{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "criss_cross = np.array([\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the robot will be painted by gray 0.8\n",
    "robot_mark = 0.5      # The current robot cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Explorobotion factor\n",
    "epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qcriss_cross(object):\n",
    "    def __init__(self, criss_cross, robot=(0,0)):\n",
    "        self._criss_cross = np.array(criss_cross)\n",
    "        nrows, ncols = self._criss_cross.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"product\" is\n",
    "        self.open_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._criss_cross[r,c] == 1.0]\n",
    "        self.open_cells.remove(self.target)\n",
    "        if self._criss_cross[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid criss_cross: target cell cannot be blocked!\")\n",
    "        if not robot in self.open_cells:\n",
    "            raise Exception(\"Invalid robot Location: must sit on a free cell\")\n",
    "        self.reset(robot)\n",
    "\n",
    "    def reset(self, robot):\n",
    "        self.robot = robot\n",
    "        self.criss_cross = np.copy(self._criss_cross)\n",
    "        nrows, ncols = self.criss_cross.shape\n",
    "        row, col = robot\n",
    "        self.criss_cross[row, col] = robot_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.criss_cross.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.criss_cross.shape\n",
    "        nrow, ncol, nmode = robot_row, robot_col, mode = self.state\n",
    "\n",
    "        if self.criss_cross[robot_row, robot_col] > 0.0:\n",
    "            self.visited.add((robot_row, robot_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in robot position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        robot_row, robot_col, mode = self.state\n",
    "        nrows, ncols = self.criss_cross.shape\n",
    "        if robot_row == nrows-1 and robot_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (robot_row, robot_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.criss_cross)\n",
    "        nrows, ncols = self.criss_cross.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the robot\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = robot_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        robot_row, robot_col, mode = self.state\n",
    "        nrows, ncols = self.criss_cross.shape\n",
    "        if robot_row == nrows-1 and robot_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.criss_cross.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.criss_cross[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.criss_cross[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.criss_cross[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.criss_cross[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qcriss_cross):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qcriss_cross.criss_cross.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qcriss_cross.criss_cross)\n",
    "    for row,col in qcriss_cross.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    robot_row, robot_col, _ = qcriss_cross.state\n",
    "    canvas[robot_row, robot_col] = 0.3   # robot cell\n",
    "    canvas[8,6 ] = 0.9 # product cell\n",
    "    \n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray',animated=True)\n",
    "    plt.savefig('books_read.png')\n",
    "    a = np.asarray(canvas)\n",
    "    np.savetxt(\"foo.csv\", a, delimiter=\",\")\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/.local/lib/python2.7/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf25595350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qcriss_cross = Qcriss_cross(criss_cross)\n",
    "canvas, reward, game_over = qcriss_cross.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qcriss_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf234c4290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qcriss_cross.act(DOWN)  # move down\n",
    "qcriss_cross.act(RIGHT)  # move right\n",
    "qcriss_cross.act(RIGHT)  # move right\n",
    "qcriss_cross.act(RIGHT)  # move right\n",
    "qcriss_cross.act(UP)  # move up\n",
    "show(qcriss_cross)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qcriss_cross, robot_cell):\n",
    "    qcriss_cross.reset(robot_cell)\n",
    "    envstate = qcriss_cross.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qcriss_cross.act(action)\n",
    "        show(qcriss_cross)\n",
    "        if game_status == 'win':\n",
    "            \n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def completion_check(model, qcriss_cross):\n",
    "    for cell in qcriss_cross.open_cells:\n",
    "        if not qcriss_cross.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qcriss_cross, cell):\n",
    "            return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d criss_cross cells info, including robot cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "n_epoch - Number of training epochs\n",
    "\n",
    "max_memory - Maximum number of game experiences we \n",
    "keep in memory (see the Experince class above)\n",
    "    \n",
    "    data_size - Number of samples we use in each training epoch. \n",
    "    \n",
    "    This is the number episodes (or game experiences) which we randomly select from our experiences repository (again, see the Experience class above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qtrain(model, criss_cross, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: criss_cross (see above)\n",
    "    qcriss_cross = Qcriss_cross(criss_cross)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_open_cells = len(qcriss_cross.open_cells)\n",
    "    hsize = qcriss_cross.criss_cross.size//2   # history window size\n",
    "    win_robote = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        robot_cell = random.choice(qcriss_cross.open_cells)\n",
    "        qcriss_cross.reset(robot_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qcriss_cross.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qcriss_cross.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qcriss_cross.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_robote = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win robote: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_robote, t))\n",
    "        plt.show(qcriss_cross)\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_robote > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qcriss_cross):\n",
    "            print(\"Reached 100%% win robote at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    model.save('Model.h5')\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('Model_saved')\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(criss_cross, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(criss_cross.size, input_shape=(criss_cross.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(criss_cross.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/14999 | Loss: 0.0405 | Episodes: 240 | Win count: 0 | Win robote: 0.000 | time: 36.9 seconds\n",
      "Epoch: 001/14999 | Loss: 0.0017 | Episodes: 227 | Win count: 0 | Win robote: 0.000 | time: 62.5 seconds\n",
      "Epoch: 002/14999 | Loss: 0.0016 | Episodes: 46 | Win count: 1 | Win robote: 0.000 | time: 67.7 seconds\n",
      "Epoch: 003/14999 | Loss: 0.0048 | Episodes: 36 | Win count: 2 | Win robote: 0.000 | time: 71.7 seconds\n",
      "Epoch: 004/14999 | Loss: 0.0035 | Episodes: 14 | Win count: 3 | Win robote: 0.000 | time: 73.3 seconds\n",
      "Epoch: 005/14999 | Loss: 0.0019 | Episodes: 14 | Win count: 4 | Win robote: 0.000 | time: 74.9 seconds\n",
      "Epoch: 006/14999 | Loss: 0.0419 | Episodes: 137 | Win count: 5 | Win robote: 0.000 | time: 90.4 seconds\n",
      "Epoch: 007/14999 | Loss: 0.1122 | Episodes: 164 | Win count: 6 | Win robote: 0.000 | time: 108.8 seconds\n",
      "Epoch: 008/14999 | Loss: 0.0131 | Episodes: 23 | Win count: 7 | Win robote: 0.000 | time: 111.4 seconds\n",
      "Epoch: 009/14999 | Loss: 0.0309 | Episodes: 28 | Win count: 8 | Win robote: 0.000 | time: 114.6 seconds\n",
      "Epoch: 010/14999 | Loss: 0.0061 | Episodes: 38 | Win count: 9 | Win robote: 0.000 | time: 118.9 seconds\n",
      "Epoch: 011/14999 | Loss: 0.0089 | Episodes: 31 | Win count: 10 | Win robote: 0.000 | time: 122.4 seconds\n",
      "Epoch: 012/14999 | Loss: 0.0122 | Episodes: 1 | Win count: 11 | Win robote: 0.000 | time: 122.5 seconds\n",
      "Epoch: 013/14999 | Loss: 0.0079 | Episodes: 9 | Win count: 12 | Win robote: 0.000 | time: 123.5 seconds\n",
      "Epoch: 014/14999 | Loss: 0.0124 | Episodes: 37 | Win count: 13 | Win robote: 0.000 | time: 127.7 seconds\n",
      "Epoch: 015/14999 | Loss: 0.0028 | Episodes: 43 | Win count: 14 | Win robote: 0.000 | time: 132.5 seconds\n",
      "Epoch: 016/14999 | Loss: 0.0174 | Episodes: 87 | Win count: 15 | Win robote: 0.000 | time: 142.3 seconds\n",
      "Epoch: 017/14999 | Loss: 0.0050 | Episodes: 11 | Win count: 16 | Win robote: 0.000 | time: 143.5 seconds\n",
      "Epoch: 018/14999 | Loss: 0.0137 | Episodes: 16 | Win count: 17 | Win robote: 0.000 | time: 145.4 seconds\n",
      "Epoch: 019/14999 | Loss: 0.0199 | Episodes: 10 | Win count: 18 | Win robote: 0.000 | time: 146.5 seconds\n",
      "Epoch: 020/14999 | Loss: 0.0160 | Episodes: 17 | Win count: 19 | Win robote: 0.000 | time: 148.5 seconds\n",
      "Epoch: 021/14999 | Loss: 0.0194 | Episodes: 2 | Win count: 20 | Win robote: 0.000 | time: 148.7 seconds\n",
      "Epoch: 022/14999 | Loss: 0.0073 | Episodes: 33 | Win count: 21 | Win robote: 0.000 | time: 152.4 seconds\n",
      "Epoch: 023/14999 | Loss: 0.0091 | Episodes: 10 | Win count: 22 | Win robote: 0.000 | time: 153.5 seconds\n",
      "Epoch: 024/14999 | Loss: 0.0028 | Episodes: 20 | Win count: 23 | Win robote: 0.000 | time: 155.7 seconds\n",
      "Epoch: 025/14999 | Loss: 0.0069 | Episodes: 10 | Win count: 24 | Win robote: 0.000 | time: 156.9 seconds\n",
      "Epoch: 026/14999 | Loss: 0.0068 | Episodes: 33 | Win count: 25 | Win robote: 0.000 | time: 161.1 seconds\n",
      "Epoch: 027/14999 | Loss: 0.0061 | Episodes: 8 | Win count: 26 | Win robote: 0.000 | time: 162.1 seconds\n",
      "Epoch: 028/14999 | Loss: 0.0028 | Episodes: 16 | Win count: 27 | Win robote: 0.000 | time: 164.2 seconds\n",
      "Epoch: 029/14999 | Loss: 0.0190 | Episodes: 12 | Win count: 28 | Win robote: 0.000 | time: 165.5 seconds\n",
      "Epoch: 030/14999 | Loss: 0.0093 | Episodes: 7 | Win count: 29 | Win robote: 0.000 | time: 166.3 seconds\n",
      "Epoch: 031/14999 | Loss: 0.0026 | Episodes: 13 | Win count: 30 | Win robote: 0.000 | time: 167.8 seconds\n",
      "Epoch: 032/14999 | Loss: 0.0026 | Episodes: 29 | Win count: 31 | Win robote: 0.000 | time: 171.2 seconds\n",
      "Epoch: 033/14999 | Loss: 0.0028 | Episodes: 20 | Win count: 32 | Win robote: 0.000 | time: 173.5 seconds\n",
      "Epoch: 034/14999 | Loss: 0.0172 | Episodes: 18 | Win count: 33 | Win robote: 0.000 | time: 175.5 seconds\n",
      "Epoch: 035/14999 | Loss: 0.0072 | Episodes: 21 | Win count: 34 | Win robote: 0.000 | time: 177.9 seconds\n",
      "Epoch: 036/14999 | Loss: 0.0044 | Episodes: 18 | Win count: 35 | Win robote: 0.000 | time: 180.0 seconds\n",
      "Epoch: 037/14999 | Loss: 0.0052 | Episodes: 13 | Win count: 36 | Win robote: 0.000 | time: 181.5 seconds\n",
      "Epoch: 038/14999 | Loss: 0.0026 | Episodes: 12 | Win count: 37 | Win robote: 0.000 | time: 182.9 seconds\n",
      "Epoch: 039/14999 | Loss: 0.0066 | Episodes: 1 | Win count: 38 | Win robote: 0.000 | time: 183.0 seconds\n",
      "Epoch: 040/14999 | Loss: 0.0031 | Episodes: 10 | Win count: 39 | Win robote: 0.000 | time: 184.1 seconds\n",
      "Epoch: 041/14999 | Loss: 0.0028 | Episodes: 15 | Win count: 40 | Win robote: 0.000 | time: 185.8 seconds\n",
      "Epoch: 042/14999 | Loss: 0.0016 | Episodes: 7 | Win count: 41 | Win robote: 0.000 | time: 186.8 seconds\n",
      "Epoch: 043/14999 | Loss: 0.0044 | Episodes: 18 | Win count: 42 | Win robote: 0.000 | time: 189.3 seconds\n",
      "Epoch: 044/14999 | Loss: 0.0023 | Episodes: 21 | Win count: 43 | Win robote: 0.000 | time: 192.3 seconds\n",
      "Epoch: 045/14999 | Loss: 0.0025 | Episodes: 8 | Win count: 44 | Win robote: 0.000 | time: 193.4 seconds\n",
      "Epoch: 046/14999 | Loss: 0.0093 | Episodes: 8 | Win count: 45 | Win robote: 0.000 | time: 194.5 seconds\n",
      "Epoch: 047/14999 | Loss: 0.0247 | Episodes: 2 | Win count: 46 | Win robote: 0.000 | time: 194.8 seconds\n",
      "Epoch: 048/14999 | Loss: 0.0044 | Episodes: 16 | Win count: 47 | Win robote: 0.000 | time: 197.3 seconds\n",
      "Epoch: 049/14999 | Loss: 0.0027 | Episodes: 9 | Win count: 48 | Win robote: 0.000 | time: 198.8 seconds\n",
      "Epoch: 050/14999 | Loss: 0.0025 | Episodes: 9 | Win count: 49 | Win robote: 0.000 | time: 200.0 seconds\n",
      "Epoch: 051/14999 | Loss: 0.0010 | Episodes: 16 | Win count: 50 | Win robote: 1.000 | time: 201.9 seconds\n",
      "Epoch: 052/14999 | Loss: 0.0030 | Episodes: 5 | Win count: 51 | Win robote: 1.000 | time: 373.5 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 053/14999 | Loss: 0.0014 | Episodes: 4 | Win count: 52 | Win robote: 1.000 | time: 8.99 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAABdBJREFUeJzt3TFq1GkcxvHfLAELE0iMYDP9/wCTA8TGNo1n8AR23kDmBDmBrXiAmQNkujTbWQzCQBpBwe7dYpXdYmEy6/sn80w+H5guPLyoX5I0PyettQKy/PHQDwB2J1wIJFwIJFwIJFwIJFwIJFwIJFwIJFwIdLTLFx8fH7fz8/O+Dzg6qrOzs66bVVXfv3+vp0+fPurdpLem7Y711s+fP9fd3d1k29ftFO75+Xm9e/fu/7/qPzx79qxev37ddbOqarlc1uXl5aPeTXpr2u5Yb724uLjX1/lRGQIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwLtdHPq69ev9enTp64PuLq66roHj8Fk2/+PO5lM3lTVm6qqs7Oz2fv377s+4PT0tJ4/f951s6rq27dvdXx8/Kh3k96atjvWW9++fVs3Nze/f+WxtXZdVddVVaenp+3jx48dnvePq6srVx5H2k16a9ruWG+9L7/jQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQqCdjsW9ePFi9uHDh64PSDoQlrab9Na03Yc+FlettXt/ZrNZ622xWHTftDvept3xNltr7WdjW1v0ozIEEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEu6eWK1WNZlMun7G2EzcPUSuPO7J7mazqfV63XVzOp1230zcHYbBlcfeki77jbk7n89bVXX9jLGZuOvKI7AXhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhLsnZrPZTve/7vMZYzNx9xC58njAu0lvTdt15THsGmPSbtJb03ZdeQR2JlwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJNw9sVqtajKZdP2MsZm4e4hcedyT3c1mU+v1uuvmdDrtvpm4OwyDK4+9JV32G3N3Pp+3qur6GWMzcdeVR2AvCBcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCORZ3wLtJb03bdSwu7Khb0m7SW9N2HYsDdiZcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCOTK457sbjabWq/XXTen02n3zcTdYRhceewt6bLfmLvz+bxVVdfPGJuJu648AntBuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBDo6KEfwN9ms1m1LYf7drVcLrtv/tr98uVL993b29vR3ntoXHk84N0x3/rkyZPuuz9+/KiTk5Puu0l/tve98rj1O25r7bqqrquqLi4u2uXl5e+/7l+Wy2X13rQ73uav3WEYuu/e3t4++j/b+/I7LgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgRy5XFPrFarevnyZdfN+XzefXPs3VevXnXfXSwW3TcfmiuPe7K72WxqvV533ZxOp903E3eHYTi4K4/VWrv3Zzabtd4Wi0X3zcTd+XzeqqrrZ4zNxN0x/s7G+nfws7GtLfodFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwLtdCyuqoaq+rPzG55X1V3nTbvjbdodb7OqamitnWz7oq3hjm0ymdy01i7s9t9Nemva7kO/1Y/KEEi4EGgfwr22O9pu0lvTdh/0rQ/+Oy6wu334jgvsSLgQSLgQSLgQSLgQ6C9uTnId0ZmtLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 054/14999 | Loss: 0.0020 | Episodes: 11 | Win count: 53 | Win robote: 1.000 | time: 12.11 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 055/14999 | Loss: 0.0027 | Episodes: 5 | Win count: 54 | Win robote: 1.000 | time: 15.18 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 056/14999 | Loss: 0.0014 | Episodes: 13 | Win count: 55 | Win robote: 1.000 | time: 18.31 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 057/14999 | Loss: 0.0033 | Episodes: 13 | Win count: 56 | Win robote: 1.000 | time: 48.05 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(criss_cross)\n",
    "qtrain(model, criss_cross, epochs=10, max_memory=8*criss_cross.size, data_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model('Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(saved_model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d13679f1b566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopen_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqcriss_cross\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_cells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqcriss_cross\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "open_cell = random.choice(qcriss_cross.open_cells)\n",
    "play_game(saved_model,qcriss_cross,(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
